---
title: AI绘画Lora训练经验
categories: AI绘画
date: 2023-06-01 00:00:00
tags: [AI,Lora]
description: Stable Diffusion Lora训练经验总结
cover: https://md-pic-1300959784.cos.ap-nanjing.myqcloud.com/img/202306012118676.png
comments: false
---

# AI绘画Lora训练经验总结

## 原理

简单来说，Lora训练的原理就是根据<font color="#3399ff">**关键词**</font>进行<font color="#3399ff">**图片区域**</font>学习和融合。

这里面有一些需要注意的点：

### Tag关键词

Tag关键词对于训练效果的影响并不是决定性的，相反可能是微乎其微，主要是 `辅助` 作用。

在学习的过程中：对于Tag中有的关键词<font color=red>**注意力下降**</font>，对于打掉的Tag关键词<font color=green>**注意力提升**</font>。

> 事实上，生成Tag的方法千奇百怪，没有一个最佳方法能应对所有场合，因此最好还是人工进行筛查。

### 训练图片选择

训练图片的内容决定了最终的大方向，也就是质量。

简单来说：Tag出问题只会导致在生成时的<font color=red>**Prompt要求更高**</font>，但是总会有好的<u>Seed</u>来出现好图。

而如果图片出现了问题，那么无论如何也难以<font color=red>**凭空创造**</font>

### 训练参数

训练参数与图片密切相关。

例如：采用抠图的训练集进行训练，需要更小的Step，因为背景几乎没有用到任何注意力。

过大的训练Step可能会导致<font color="#5500DD">**颜色迁移**</font>，也就是说的学到了 `画风` ，这可以算是 <u>过拟合</u> 了。

## 经验

### Tag关键词

最省事的应该就是Tag关键词部分，使用VIT模型直接进行图片识别，之后用Tag插件打掉部分 <font color=red>**头发**</font> 或 <font color=red>**眼睛**</font> 相关的Tag，总花费可能不到10分钟。

#### 触发词

实际上，Lora训练中经常会提到一个<font color=red> **“触发词”**</font>，也就是加入一个固定的 <font color=red>**无意义Tag**</font>，将图片中的人物特征与触发词绑定。通过多个触发词的设定，可以将多个同人物的各种服装混合训练，并通过关键词区分。

在实际的实验中发现，触发词效果并不明显，例如：将 ”美杜莎人形“ 和 ”美杜莎蛇形“ 两者混在一起，通过触发词 *“美杜莎，美杜莎人形”，“美杜莎，美杜莎蛇形”* 进行区分，那么训练出来的结果会出现**二者之间**的情况，也就是特征混合。

这也很好理解，一个关键词没有那么强的特性来约束整个画面，因为往往将VIT识别后的Tag直接生成图片，与原图风格是类似的，区别可能很小，这也就是需要学习的地方。

总的来说，麻烦并且效果不好，还是分开训练更方便，同时 `单一触发词基本也没什么影响` ，因此无需在意这一点。

#### <font color=red>0821更新</font>

<font color="#FF4500">**触发词**</font>对结果的影响程度

> 在尝试多个触发词形成**1分类**，**2分类**的实验中发现。Lora在使用**AdamW8**优化器时，可以实现几乎**不过拟合** `也就是无论图片多少，训练多少轮，最终的改变都较小`。因此，设置多个触发词效果并不好，会冲突得很厉害。Lora更像是一个修饰器，主体还是由SD大模型决定，因此无法真正学习到两种衣服，或者说学习的效果不好。
>
> 如果某一个服装的资源很少，可能就出现了一个场景，并且觉得那么**没有必要再新建一个Lora**，也可以放在一起训练，提升该Lora的丰富度。
>
> <font color="#FF4500">**由触发词产生的灵感**</font>
>
> 前面说到Lora的影响很有限制，因此在不过拟合的情况下，Lora仅仅是一个修饰。
>
> 例如：在训练战甲之类的角色时，在WD1.4中会识别出大量的 `armor`，`science fiction` 等，这些词其实就是**触发词**，也就是通过这些词，使得大模型可以产生相应的基础图，后通过Lora修饰出想要的图片，如果在使用Lora时不加入这些词，那么会出现 **服装变化**，也就是没有战甲了 `灵活性很强`。
>
> 因此，在训练Lora时，大可不必去打标，因为高重复性的词就是 **”触发词“**，删掉比如 ”长发，马尾辫“ 的话，在后面如果不加入这些词，依然会被其他Tag改掉。<font color=green>**不删的目的主要是方便查看**</font>
>
> **触发词的位置很重要**，必须要放在 `<lora: :n>` 标签的前面，前面的词主要是Lora中有的词，通过Lora进行修饰，而后面的词主要是自己想要的动作或服饰。换句话说，<font color=blue>前是决定角色的整体服装，lora后是为了更多的动作和服装变化</font>，比如，一个机甲角色，lora前加 `science fiction`，一个国风角色，lora前加 `chinese clothes` 。因此，想要利用好Lora，也需要把握好lora资源中的WD结果，因为，就算把Tag打掉，触发词依然不会失效。<font color="#FF4500">最终的原因应该是AdamW8的防过拟合效果，保证了更多的灵活性</font>
>
> *备注问题：*
>
> 机甲角色不好训练，因为局部是无法识别的，比如机甲鞋子特写，看着像是长方形棱柱，WD1.4可能会识别为武器什么的，大模型中这些信息很少，如果训练得多了，会出现一些桌子腿，椅子腿变成了机甲的鞋子，因此，WD识别不了的内容，尽量不要进行使用，不然可能会适得其反。

#### Tag的重要性

Tag虽然不能决定大局，但是对于后期Prompt的要求有很大的影响，例如：在 ”须灵犀” 角色的训练中，由于图片大多是微微低头失落表情，结果由于眼睛比较大，被VIT模型识别为了 “looking at viewer”，最终在生成时，“looking at viewer” 就出现了小眼睛图片，很没精神。

再比如，在 ”美杜莎人形“ 的训练中，由于出现了很多正脸图，也是很好的 “looking at viewer”，但是由于正脸图片过多，导致这个关键词的特征学习过深，最终结果就是一旦生成时出现 ”looking at viewer“，就会明显有色彩迁移，也就是原图的`黑色脸`，因此可能需要降低关键词权重（此时降低Lora权重是无效的，因为Lora中的特征包含了颜色，降低会使得脸都不像）。



### 图片选择

首先摒弃抠图的方式，太过繁琐，但是<font color=red>**需要修图**</font>，不然字幕水印是会学习到的。

图片自然是越多越好，但是需要进行权重平衡，例如某人说的：“人物头像图一定要多，要占一半”，实际测试后发现，相似的图片不能多，这样会导致前面出现的关键词强绑定现象。在 `头像，半身，全身`方面一定要尽可能平衡，同时 <font color=red>**正脸不能过多**</font>，正面不能过多，注意 “looking at viewer” 过多的问题。

> 这样来想，生成模型学习的是 **关键词** 和 **特征** 的关系，也就是说，你的大脸图只有在你需要生成大脸图的时候才会用到，给再多也不能在全身图上加太多效果，反而使得全身图学习的东西减少，也就是注意力偏移。
>
> 而全身图想要面部完好，需要很多高清全身图和半身图，这才有可能出现不错的效果。简而言之，*<u>任何一个图类过多，都会导致其他类注意力偏移</u>*。

总的来说，权衡并不是仅仅看头像，半身，全身等这些方面来说的，而是看相似度，最完美的情况肯定是把模型转一圈截出的图片，<font color=green>**不能看着清晰就加很多的相似图片**</font>，这很容易导致不平衡（误差不小，但是有色彩迁移）。

#### <font color=red>0708更新</font>

<font color="#FF4500">**图片选取**</font>对结果的影响程度

> 在尝试使用各种筛选方法【筛去整体较模糊图片，间隔n帧抽取一次，比较帧间相似度】后，往往训练后效果都不错。
>
> 这就出现一个问题 **费老大劲得到的高质量图片究竟是否有比较明显的效果呢** ，实际上，多图少图，高质量低质量，总体的分布是不变的，也就是说：**整体的效果是相似的**，差别只会出现在一些细节情况中。
>
> 比如，如果训练集中**没有鞋子**的图片，那么在生成含有鞋子的图时，通常会变为**默认**，甚至会有点**扭曲**。也就是说：好的训练集只会在小范围中胜出，大部分情况下是相似的，因此，数据集更多的应该是丰富，而不在多，也就是说，相似的镜头人物要去重，而稀少的镜头一定要保留。<font color=green>**丰富度越多越好，而不是图片越多越好。**</font>

#### <font color=red>0805更新</font>

<font color="#FF4500">**图片数据集构建技巧**</font>

> 之前提到了<font color=green>**丰富度越多越好，而不是图片越多越好。**</font>这句话确实没有问题，但是实际上很多角色没有那么多 `丰富` 的图片素材。导致图片单调，比如只有一个 **服装，姿势** 的图，可能会出现生图难以调节。也就是 **过拟合**。
>
> ”过拟合” 并不是训练步数太多导致的。它更像是一个数学计算过程：
>
> **目标角色一个服装、姿势的图片就是一个关于x的方程**
>
> 如果我们的**数据集**只有一个 `关于x的一元一次` 方程，那么结果必然是很小的一个**点空间**；
>
> 但是如果加入了一些不相关的内容，也就是加入一个关于y的方程，那么结果会是一个**线空间**；
>
> 同理可知，加入越多的不相似 z<sub>1</sub>,z<sub>2</sub>,z<sub>3</sub>... 内容，这个解空间就会越大，同时会**保证x的精确度**。
>
> **训练的步数影响：**
>
> 训练的过程就像是在寻找 <u>解空间</u> 的过程，如果是简单的只有x的一元方程，那么就算再少的步数也能过拟合了，空间太小；反之，一旦解空间本身很大，就算步数多了一点，也难以 “过拟合” 。
>
> **综上所示，过拟合的主要原因是素材单一，解决办法应该是加相似图，或者有关图，而不需要调整训练步数。**

#### <font color=red>0831更新</font>

<font color="#FF4500">**图片数据集构建误区**</font>

> 前面说到构建训练集图片时，要加入一些噪声，使得过拟合减少。因此 <font color="#689b0">**投机取巧**</font> ，误以为噪声就是质量不高的图片，比如：<font color="#1a93cb"><u>**超小图，模糊图（包括运动模糊和焦点模糊）**</u></font>。
>
> 实际上，我们训练模型由于 **AdamW8** 以及 **金字塔噪声**，已经大大避免了过拟合。剩下的只需要考虑精度问题即可。训练后的模型不可能比训练集的图片清晰度更高，因此如果出现 **单Lora生成图片模糊** 的情况，大概率是因为训练图片中模糊图过多。`这种情况实际上是欠拟合，也就是无解，解空间无限大，无论训练多久，loss基本不下降。`
>
> 模糊图产生的原因有很多，实际上，动漫视频中的大多数1080P都比较模糊，再加上后期各种平台的超分，从1080P简单扩展到4K，实际上会使得图片更加模糊，也就是 `blurry` 。在这种情况下，训练图片数据应该更注重清晰度，不仅要是 **【大图】**，还要尽可能**【静态】**，此外还要**【聚焦】**。==由于3D渲染压力，制作时通常会仅聚焦一个角色，既缓解压力又模拟出人眼效果。==
>
> **此外，环境光对于模型的影响很大**，例如人物面部有 **偏色的光源** 或 **阴影**，在实际生成时，面部画面可能会偏色，这也就是我们不想要的 <font color="fc3d49">**光源** 和 **影子**</font>，因此在选图上，需要**狠心做取舍**。
>
> 题外=》训练轮数也需要提升，仅10轮难以学习到一些 `forehead mark` 之类的浅色细节。
>
> **总结**：由于训练手法流程已经纯熟，避免了**过拟合**现象，但是引入了**欠拟合**。因此我们人工的目标就是使得训练loss能下降尽可能多，大概是2个格子以上，3个格子左右（一格0.01）。
>
> ```markdown
> 实验证明，在避免过拟合的机制下，训练时loss越低，生图时自定义Tag效果越好。
> 对于loss较高的，自定Tag效果很小，需要加很高的权重才行，这是是学习模糊后果。
> ```

#### <font color=red>0907更新</font>

> **上面提到** =》” 图片需要尽可能 `大图`，尽可能 `清晰` ”
>
> 实际上，出现 **“ 出图模糊 ，不好控制“** 情况 <font color=violet>**不一定是图片质量问题**</font>
>
> 还与选择图片的 可识别度有关，<font color=lightCoral>单纯的 **blurry**(**模糊**) 对图片影响不大</font>，就像**weapon**(**武器**)，是可以去除的。更深层的原因是 <font color=dodgerblue>动漫**画风**的区别，人物**穿着**的区别</font>。
>
> - 有些动漫由于年份较早，画风本身就有一些雾化，也就是<u>微微blurry</u>，此时无论如何增加**epoch**(**轮数**)，其极限也就是画风相似的<u>微微blurry</u>。==【如果要高清，需要调节SD模型的Prompt，固定画风】==
> - 有些动漫角色是不常见的**盔甲，战装，科幻**。这种类型在基础SD模型中很少数据，因此训练起来难度比较大。同时需要着重考虑**【语义分歧】**，比如给一张 `战靴高清特写` ，由于过于科幻，因此无法识别为**`shoes` 或 `boots`**，可能最后变成了`桌角` 。
> - 有些**日常装束的角色**，例如大裙子会很好训练，随便几十张图就可以出完美图，并且灵活性很高。
> - 一些**发型，面部印记**没有专门的tag指定，只能通过增加【**训练轮数**】来还原。
>
> 【**图片选择**】注意点：
>
> 图片应该尽可能全面，<font color=red>可以模糊，但是不能失真</font>。（要好看模糊图，不要扭曲清晰图）
>
> 图片应该注意特写，可以有，但是不能多，<font color=red>保证语义清晰</font>。（鞋子最好带一点腿）
>
> 图片应该控制丰富度，不能说是这个角色的就可以，<font color=red>对于顶部镜头和底部镜头要少</font>。
>
> 图片应追求美感，如果是脸扭曲变形，或者面部斜脸，广角畸变，即使清晰依然不好
>
> 图片应追求干净，找图最好从多个场景中选择，这样可以排除**background**固定。
>
> 图片应注意角色身上的光源，有的是**黄色（火光）**，有的是**白色（月光，阳光）**，还有的是**红光（特效）**。这些都会体现在出图的过程中。
>
> 图片应注意角色身上的阴影，如：别人的阴影打在角色身上 √，物体的阴影打在角色身上 √，角色自身服饰发型造成的阴影 ×，尽量消除掉。

### 训练参数

一般来说，图片少倍数多；图片多倍数少，以前还按照训练最终的Loss值来进行评判。但是实际测试下来，loss值并不绝对，通常 `2000` Step（Batch Size = 7）中的Loss值会下降0.1，起始根据图片来定，如果是抠图白色背景，那么会很低；复杂背景，会比较高；不同图片很多，则会更高。因此，看训练损失基本没用。

`2000`Step是经过测试的，只要没有出现色彩迁移，基本都不会过拟合。如果**原图片数量很少**，需要大的倍数，那么需要**更加注意权重平衡的问题**。<font color=red>X</font>

> 这也就是为什么说 图只要无敌多（小倍数）啥也不怕，因为权重影响不大，并且不同的背景就够分散注意力了。

#### <font color=red>0624更新</font>

<font color="#FF4500">**训练步数**</font>很重要，一般1500可以保证很少的过拟合，2000Step在weight=0.7时还是有些过拟合的。

> 训练步数为什么重要？有人说：“过拟合就使用时减少weight值就行了呗”，但是实际上Lora的数据是有限的，如果weight值过小，则Lora的微调也更小， 一定会更加的不像。因此，0.7~0.5 weight时如果效果不好，就说明训练有问题。

**纠正之前的观点**，**如果图片无敌多，那么还是需要尽量平衡**

因为Lora的数据有限，因此如果一些特写，比如<u>鞋子类</u>的图很少，那么在0.7weight时大量的是面部调整和服装调整，对于鞋子基本没有效果。即使将鞋子原图Tag放进去，也需要weight到1.0时发生扭曲后才能显示。因此，对于这部分需要进行<font color="#FF4500">**平衡**</font>，以达到面部，服装，鞋子都能够很好的还原出来。

#### <font color=red>0907更新</font>

> 【训练步数】 <font color=#ef6343>**具体情况具体分析**</font>。如果细节不到位，需要一点一点的加epoch，也就是step。
>
> 如果有画风变化就减少step。通常`1500step`不会过拟合，比较大众化，但是也可能会不够。

#### <font color=red>0913更新</font>

> <font color=red>【**训练步数 <u>分析</u> 与 <u>选择</u> **】</font>
>
> 通常在不超过`1500step`下默认训练一模
>
> 如果生成图片 ==(不加 \<hiqcgbody>)== 出图效果**细腻，不出现拖影** <font color=#ef6343>（通常4K分辨率训练集来训练容易实现）</font>，则说明训练步数 **【足够】**;
>
> 接下来判断 **<font color="#1d9acd">是否过拟合</font>**，添加一些基础触发词，选择一些**经典图片**反推tag，之后带入生图，直到**角色动作相似**。分情况讨论：
>
> - 如果动作还原很容易实现，则说明训练 `step` 比较合适
> - 如果通过调整tag权重，可以实现大致效果，则说明训练 `step` 还算可以
> - 如果无论如何调整，都没法还原经典图效果，则说明 **过拟合**。==（人物动作和服饰固化）==
>
> 过拟合处理：
>
> - 减少图片 `repeat` 参数==（减半）==
> - 减少 `epoch` 参数
>
> 过拟合原因：
>
> - 训练集质量高，清晰度好，不需要 **1500step**
> - 训练集动作单一，容易学习。
> - 训练集服装固定，导致服饰过拟合
>
> **注意**：动作、服饰过拟合<font color="orange">**不能通过减小lora的weight来解决**</font>，因为通过减少weight后会导致面部严重欠拟合，整体依然是不自然。
>
> <font color="#0272a7">**新思想：图片多少都能训练处不错的效果。**图片多固然好，但是 **少图+少step **效果依然不差；有时候 **多图** 依然要 **少step** 因此 **<u>step选取很灵活</u>**</font>
>
> 

## 底模选择

目前主要是在 <font color=fuchsia>**完美世界V1**</font>上训练Lora，它本身是一种2D的优化风格，所以需要加上0.3的 <font color="FF8888">**hipoly**</font> Lora，此时图片有了3D效果，之后在此基础上进行修改。

这正验证了前面说的，训练的Lora学习的是微小的，它不能定基调，需要你调好整体基调后，把Lora打上微调一下。也就是说<u>*无Lora时的状态才是整体的一个风格*</u>。当然，如果出现色彩迁移，那么就会影响整体基调，这就不好了。

***后续会将新训练的结果进行图片记录，以方便对比总结更多经验***

## <font color=red>2023/11/12终更新</font>

过拟合处理：

- 图片增强：翻转，调彩，裁剪
- dropout
- 加噪声

欠拟合处理：

- 取消图片增强
- dropout
- 减噪声

优化方法：

- PS图片调色，尽量一致无色光
- 素材过少，考虑只训练头部，身体可以随意调整
- 训练数据100~200即可，要求高质量，清晰，完美

实际效果：

- 与训练底膜相关

- 与图片的人物服装，背景相关
- 与图片画风线条，色彩相关
- 与图片比例（头，身体）相关
- 与训练参数相关、

**<font color=red size=4>\*\*不同的训练数据，需要尝试不同的训练方式</font>**

**<font color=red size=4>\*\*没有唯一的定律，训练效果与人工付出成正比</font>**



