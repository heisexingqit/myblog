---
title: AI绘画Lora训练经验
categories: AI绘画
date: 2023-06-01 00:00:00
tags: [AI,Lora]
description: Stable Diffusion Lora训练经验总结
cover: https://md-pic-1300959784.cos.ap-nanjing.myqcloud.com/img/202306012118676.png
comments: false
---

# AI绘画Lora训练经验总结

## 原理

简单来说，Lora训练的原理就是根据<font color="#3399ff">**关键词**</font>进行<font color="#3399ff">**图片区域**</font>学习和融合。

这里面有一些需要注意的点：

### Tag关键词

Tag关键词对于训练效果的影响并不是决定性的，相反可能是微乎其微，主要是 `辅助` 作用。

在学习的过程中：对于Tag中有的关键词<font color=red>**注意力下降**</font>，对于打掉的Tag关键词<font color=green>**注意力提升**</font>。

> 事实上，生成Tag的方法千奇百怪，没有一个最佳方法能应对所有场合，因此最好还是人工进行筛查。

### 训练图片选择

训练图片的内容决定了最终的大方向，也就是质量。

简单来说：Tag出问题只会导致在生成时的<font color=red>**Prompt要求更高**</font>，但是总会有好的<u>Seed</u>来出现好图。

而如果图片出现了问题，那么无论如何也难以<font color=red>**凭空创造**</font>

### 训练参数

训练参数与图片密切相关。

例如：采用抠图的训练集进行训练，需要更小的Step，因为背景几乎没有用到任何注意力。

过大的训练Step可能会导致<font color="#5500DD">**颜色迁移**</font>，也就是说的学到了 `画风` ，这可以算是 <u>过拟合</u> 了。

## 经验

### Tag关键词

最省事的应该就是Tag关键词部分，使用VIT模型直接进行图片识别，之后用Tag插件打掉部分 <font color=red>**头发**</font> 或 <<font color=red>**眼睛**</font> 相关的Tag，总花费可能不到10分钟。

#### 触发词

实际上，Lora训练中经常会提到一个<font color=red> **“触发词”**</font>，也就是加入一个固定的 <font color=red>**无意义Tag**</font>，将图片中的人物特征与触发词绑定。通过多个触发词的设定，可以将多个同人物的各种服装混合训练，并通过关键词区分。

在实际的实验中发现，触发词效果并不明显，例如：将 ”美杜莎人形“ 和 ”美杜莎蛇形“ 两者混在一起，通过触发词 *“美杜莎，美杜莎人形”，“美杜莎，美杜莎蛇形”* 进行区分，那么训练出来的结果会出现**二者之间**的情况，也就是特征混合。

这也很好理解，一个关键词没有那么强的特性来约束整个画面，因为往往将VIT识别后的Tag直接生成图片，与原图风格是类似的，区别可能很小，这也就是需要学习的地方。

总的来说，麻烦并且效果不好，还是分开训练更方便，同时 `单一触发词基本也没什么影响` ，因此无需在意这一点。

#### Tag的重要性

Tag虽然不能决定大局，但是对于后期Prompt的要求有很大的影响，例如：在 ”须灵犀” 角色的训练中，由于图片大多是微微低头失落表情，结果由于眼睛比较大，被VIT模型识别为了 “looking at viewer”，最终在生成时，“looking at viewer” 就出现了小眼睛图片，很没精神。

再比如，在 ”美杜莎人形“ 的训练中，由于出现了很多正脸图，也是很好的 “looking at viewer”，但是由于正脸图片过多，导致这个关键词的特征学习过深，最终结果就是一旦生成时出现 ”looking at viewer“，就会明显有色彩迁移，也就是原图的`黑色脸`，因此可能需要降低关键词权重（此时降低Lora权重是无效的，因为Lora中的特征包含了颜色，降低会使得脸都不像）。

### 图片选择

首先摒弃抠图的方式，太过繁琐，但是<font color=red>**需要修图**</font>，不然字幕水印是会学习到的。

图片自然是越多越好，但是需要进行权重平衡，例如某人说的：“人物头像图一定要多，要占一半”，实际测试后发现，相似的图片不能多，这样会导致前面出现的关键词强绑定现象。在 `头像，半身，全身`方面一定要尽可能平衡，同时 <font color=red>**正脸不能过多**</font>，正面不能过多，注意 “looking at viewer” 过多的问题。

> 这样来想，生成模型学习的是 **关键词** 和 **特征** 的关系，也就是说，你的大脸图只有在你需要生成大脸图的时候才会用到，给再多也不能在全身图上加太多效果，反而使得全身图学习的东西减少，也就是注意力偏移。
>
> 而全身图想要面部完好，需要很多高清全身图和半身图，这才有可能出现不错的效果。简而言之，*<u>任何一个图类过多，都会导致其他类注意力偏移</u>*。

总的来说，权衡并不是仅仅看头像，半身，全身等这些方面来说的，而是看相似度，最完美的情况肯定是把模型转一圈截出的图片，<font color=green>**不能看着清晰就加很多的相似图片**</font>，这很容易导致不平衡（误差不小，但是有色彩迁移）。

#### <font color=red>0708更新</font>

<font color="#FF4500">**图片选取**</font>对结果的影响程度

> 在尝试使用各种筛选方法【筛去整体较模糊图片，间隔n帧抽取一次，比较帧间相似度】后，往往训练后效果都不错。
>
> 这就出现一个问题 **费老大劲得到的高质量图片究竟是否有比较明显的效果呢** ，实际上，多图少图，高质量低质量，总体的分布是不变的，也就是说：**整体的效果是相似的**，差别只会出现在一些细节情况中。
>
> 比如，如果训练集中**没有鞋子**的图片，那么在生成含有鞋子的图时，通常会变为**默认**，甚至会有点**扭曲**。也就是说：好的训练集只会在小范围中胜出，大部分情况下是相似的，因此，数据集更多的应该是丰富，而不在多，也就是说，相似的镜头人物要去重，而稀少的镜头一定要保留。<font color=green>**丰富度越多越好，而不是图片越多越好。**</font>

#### <font color=red>0805更新</font>

<font color="#FF4500">**图片数据集构建技巧**</font>

> 之前提到了<font color=green>**丰富度越多越好，而不是图片越多越好。**</font>这句话确实没有问题，但是实际上很多角色没有那么多 `丰富` 的图片素材。导致图片单调，比如只有一个 **服装，姿势** 的图，可能会出现生图难以调节。也就是 **过拟合**。
>
> ”过拟合” 并不是训练步数太多导致的。它更像是一个数学计算过程：
>
> **目标角色一个服装、姿势的图片就是一个关于x的方程**
>
> 如果我们的**数据集**只有一个 `关于x的一元一次` 方程，那么结果必然是很小的一个**点空间**；
>
> 但是如果加入了一些不相关的内容，也就是加入一个关于y的方程，那么结果会是一个**线空间**；
>
> 同理可知，加入越多的不相似 z<sub>1</sub>,z<sub>2</sub>,z<sub>3</sub>... 内容，这个解空间就会越大，同时会**保证x的精确度**。
>
> **训练的步数影响：**
>
> 训练的过程就像是在寻找 <u>解空间</u> 的过程，如果是简单的只有x的一元方程，那么就算再少的步数也能过拟合了，空间太小；反之，一旦解空间本身很大，就算步数多了一点，也难以 “过拟合” 。
>
> **综上所示，过拟合的主要原因是素材单一，解决办法应该是加相似图，或者有关图，而不需要调整训练步数。**

### 训练参数

一般来说，图片少倍数多；图片多倍数少，以前还按照训练最终的Loss值来进行评判。但是实际测试下来，loss值并不绝对，通常 `2000` Step（Batch Size = 7）中的Loss值会下降0.1，起始根据图片来定，如果是抠图白色背景，那么会很低；复杂背景，会比较高；不同图片很多，则会更高。因此，看训练损失基本没用。

`2000`Step是经过测试的，只要没有出现色彩迁移，基本都不会过拟合。如果**原图片数量很少**，需要大的倍数，那么需要**更加注意权重平衡的问题**。<font color=red>X</font>

> 这也就是为什么说 图只要无敌多（小倍数）啥也不怕，因为权重影响不大，并且不同的背景就够分散注意力了。

#### <font color=red>0624更新</font>

<font color="#FF4500">**训练步数**</font>很重要，一般1500可以保证很少的过拟合，2000Step在weight=0.7时还是有些过拟合的。

> 训练步数为什么重要？有人说：“过拟合就使用时减少weight值就行了呗”，但是实际上Lora的数据是有限的，如果weight值过小，则Lora的微调也更小， 一定会更加的不像。因此，0.7~0.5 weight时如果效果不好，就说明训练有问题。

**纠正之前的观点**，**如果图片无敌多，那么还是需要进行平衡的**

因为Lora的数据有限，因此如果一些特写，比如鞋子类的图很少，那么在0.7weight时大量的是面部调整和服装调整，对于鞋子基本没有效果。即使将鞋子原图Tag放进去，也需要weight到1.0时发生扭曲后才能显示。因此，对于这部分需要进行<font color="#FF4500">**平衡**</font>，以达到面部，服装，鞋子都能够很好的还原出来。



## 底模选择

目前主要是在 <font color=fuchsia>**完美世界V1**</font>上训练Lora，它本身是一种2D的优化风格，所以需要加上0.3的 <font color="FF8888">**hipoly**</font> Lora，此时图片有了3D效果，之后在此基础上进行修改。

这正验证了前面说的，训练的Lora学习的是微小的，它不能定基调，需要你调好整体基调后，把Lora打上微调一下。也就是说<u>*无Lora时的状态才是整体的一个风格*</u>。当然，如果出现色彩迁移，那么就会影响整体基调，这就不好了。

***后续会将新训练的结果进行图片记录，以方便对比总结更多经验***
